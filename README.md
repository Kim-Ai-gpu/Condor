# CondorğŸ¦…: Fast-Converging Neural Attention via Learnable Connection Functions

PyTorch implementation of "Condor: Neural Connection Networks for Enhanced Attention" 

**Key Result**: Achieves 50x better perplexity (13.77 vs 717.88) compared to standard Transformer on WikiText-2 with only 3 epochs of training.

## ğŸš€ Quick Start

```bash
# Clone repository
git clone https://github.com/Kim-Ai-gpu/Condor
cd Condor

# Install dependencies
pip install torch torchvision transformers datasets

# Run experiment
python condor.py
```

## ğŸ“Š Results

| Model | Perplexity | Epochs | Convergence |
|-------|------------|--------|-------------|
| Standard Transformer | 717.88 | 3 | âŒ Not converged |
| **Neural KY-Attention** | **13.77** | **3** | âœ… **Fast convergence** |

## ğŸ”§ Architecture

Neural KY-Attention extends traditional self-attention with learnable connection functions:

- **Learnable Connection Networks**: Each attention head learns unique connection patterns
- **Linear Complexity**: O(LÃ—WÃ—H) instead of O(LÂ²Ã—H) 
- **Window-based**: Efficient local attention with global context

## ğŸ¯ Key Features

- âš¡ **Extremely fast convergence** - achieves good performance in just 3 epochs
- ğŸ§  **Novel mathematical foundation** - based on Kim-Youngseong (KY) Transform theory
- ğŸ“ˆ **Better sample efficiency** - learns more from limited data
- ğŸ”§ **Easy to integrate** - drop-in replacement for standard attention

## ğŸ“ Citation

```bibtex
@article{kim2025condor,
  title={Condor: Condor: Neural Connection Networks for Enhanced Attention},
  author={Kim-Youngseong},
  journal={arXiv preprint arXiv:2025.xxxxx},
  year={2025}
}
```

## ğŸ“„ Paper

- **arXiv**: [Link coming soon]
- **Code**: You're looking at it! ğŸ‰

## ğŸ›  Requirements

- Python 3.8+
- PyTorch 1.9+
- transformers
- datasets

## ğŸ¤ Contributing

Issues and pull requests are welcome! This is research code, so expect some rough edges.

## ğŸ“§ Contact

Kim-Youngseong - dafaafafaf33@gmail.com

## ğŸ“œ License

MIT License - feel free to use for research and commercial applications!
